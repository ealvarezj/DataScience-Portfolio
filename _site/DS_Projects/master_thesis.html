<!DOCTYPE html>
<html lang="en-US">
  <head>
    <meta charset="UTF-8">

<!-- Begin Jekyll SEO tag v2.7.1 -->
<title>Master Thesis (Data Science Project) | Data Science Portfolio</title>
<meta name="generator" content="Jekyll v3.9.0" />
<meta property="og:title" content="Master Thesis (Data Science Project)" />
<meta property="og:locale" content="en_US" />
<meta name="description" content="Elias Alvarez Jaramillo" />
<meta property="og:description" content="Elias Alvarez Jaramillo" />
<link rel="canonical" href="http://localhost:4000/DS_Projects/master_thesis.html" />
<meta property="og:url" content="http://localhost:4000/DS_Projects/master_thesis.html" />
<meta property="og:site_name" content="Data Science Portfolio" />
<meta name="twitter:card" content="summary" />
<meta property="twitter:title" content="Master Thesis (Data Science Project)" />
<script type="application/ld+json">
{"@type":"WebPage","headline":"Master Thesis (Data Science Project)","url":"http://localhost:4000/DS_Projects/master_thesis.html","description":"Elias Alvarez Jaramillo","@context":"https://schema.org"}</script>
<!-- End Jekyll SEO tag -->

    <link rel="preconnect" href="https://fonts.gstatic.com">
    <link rel="preload" href="https://fonts.googleapis.com/css?family=Open+Sans:400,700&display=swap" as="style" type="text/css" crossorigin>
    <meta name="viewport" content="width=device-width, initial-scale=1">
    <meta name="theme-color" content="#157878">
    <meta name="apple-mobile-web-app-status-bar-style" content="black-translucent">
    <link rel="stylesheet" href="/assets/css/style.css?v=04395c5c85b38a700f57b85d3c11140f789b2e83">
    <!-- start custom head snippets, customize with your own _includes/head-custom.html file -->

<!-- Setup Google Analytics -->



<!-- You can set your favicon here -->
<!-- link rel="shortcut icon" type="image/x-icon" href="/favicon.ico" -->

<!-- end custom head snippets -->

    
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.css" integrity="sha384-yFRtMMDnQtDRO8rLpMIKrtPCD5jdktao2TV19YiZYWMDkUR5GQZR/NOVTdquEx1j" crossorigin="anonymous">
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/katex.min.js" integrity="sha384-9Nhn55MVVN0/4OFx7EE5kpFBPsEMZxKTCnA+4fqDmg12eCTqGi6+BB2LjY8brQxJ" crossorigin="anonymous"></script>
    <script defer src="https://cdn.jsdelivr.net/npm/katex@0.10.2/dist/contrib/auto-render.min.js" integrity="sha384-kWPLUVMOks5AQFrykwIup5lo0m3iMkkHrD0uJ4H5cjeGihAutqP0yW0J6dpFiVkI" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>
  </head>
  <body>
    <a id="skip-to-content" href="#content">Skip to the content.</a>

    <header class="page-header" role="banner">
      <h1 class="project-name">Master Thesis (Data Science Project)</h1>
      <h2 class="project-tagline">Elias Alvarez Jaramillo</h2>
      
        <a href="https://github.com/ealvarezj/DataScience-Portfolio" class="btn">View on GitHub</a>
        <a href="/" class="btn">Projects</a>
      
      
    </header>

    <main id="content" class="main-content" role="main">
      <h1 id="master-thesis-data-science-project">Master Thesis (Data Science Project)</h1>

<p>This project contains procedures regarding my master thesis: <strong>“Design and implementation of a machine learning model for the prediction of production times, as an optimization tool for proposal management tasks for die forged products.”</strong>. This has been my most comprenhensive Data Science Project until now. Due to a nondisclosure agreement, I am not able to share any data or code regarding the project. For this reason I am only going to provide information about the procedure used in the project.</p>

<hr />
<h2 id="introduction">Introduction</h2>

<p>Already in the development phase of new high-quality components, engineers are faced with the question of the relationship between function and manufacturing costs. Often, the intended use case and the resulting geometric design of a component determine which manufacturing process might make sense from a production and commercial point of view. The decision-making processes can take a long time, which unnecessarily prolongs the development time of products. One of the main aims in the product development department is to provide customers’ development engineers with the most accurate possible cost, requirement, and quality-related advantages of the closed-die forging process to help facilitate quick decision-making. 
A high degree of technical and experiential knowledge is required to meet this standard, which is already applied during the pre-calculation phase through the development of technically sophisticated workflow sequences. It quickly becomes clear what an immense effort hides behind the processing of a customer product request. Information from the most diverse production areas must be collected repeatedly and evaluated and classified concerning its relevance for the best possible manufacturing process.</p>

<p>This work tries to tackle, employing <code class="language-plaintext highlighter-rouge">Supervised Machine Learning</code>, the proposal management process in product development. It should be possible to noticeably accelerate the processing time of the pre-calculation process with machine learning methods and at the same time maintain a high quality of the proposal management results. This use case aims to reduce the dependency on Empirical Research for assignment of production times, particularly for the forging process of closed die forged parts, eliminating the need for FEM Simulations to back up first-time proposals.</p>

<p><code class="language-plaintext highlighter-rouge">Closed die forging</code> belongs according to the German Institute for Standardisation DIN to the pressing forming processes besides indentation, rolling and open die forging. For the forging process, forging dies with the negative geometries are used. A preheated ingot is put between the dies, force is applied (e.g. forging hammer), the material starts to flow inside the die until the end geometry is filled.</p>

<table>
<thead>
<tr>
    <th>Hydraulic Counterblow Hammer</th>
    <th>Closed die forging process</th>
</tr>
</thead>
<tbody>
<tr>
    <td><img src="https://www.dango-dienenthal.de/fileadmin/user_upload/applications/close-die-forging/DD-Gesenkschmiede_1.jpg" alt="hammer" width="400" /></td>
    <td><img src="https://slideplayer.org/slide/1343508/3/images/12/Gesenkschmieden-Prinzipdarstellung%3A.jpg" alt="forming_process" width="400" /></td>
</tr>
</tbody>
</table>

<p>Due to the way materials and especially steel aloys behave in the plasticity state (this is the state in which material deforms and can not return to its initial state) it is not possible using a generalized formula to calculate the required time for the forging process. The plastic deformation behavior from materials cannot be linearly explained. Depending on temperature, chemical composition of the material at varying strain rates, the amount of force needed to achieve the desired true strain varies. Below is a Comparison of flow curves for a C15 steel, with different temperatures and strain
rates.</p>

<figure class="image">
    <p align="center">
        <img src="2021-09-29-14-26-06.png" alt="Comparison of flow curves for a C15 steel, with different temperatures and strain
rates." width="500" />     
    </p>
    <div style="text-align:center">
        <figcaption class="figure-caption text-center"><i>Comparison of flow curves for a C15 steel, with different temperatures and strain
rates.</i>
        </figcaption>
    </div>
</figure>

<p>Until now, the prediction of the materials’ behaviour on the parts’ geometry can only be achieved using FEM (Finite element method) simulations. The idea of this project is to find a faster way of achieving a prediction of the forging time for which the deformation force plays an important role.</p>

<h2 id="data-availability">Data availability</h2>

<p>Data availability has become nowadays a topic of high relevance for most companies. Technologies like Data Mining are used to find interesting patterns in data that might well already been there, stored and archived or being generated on a daily basis. The learning dataset is composed of data coming from different sources. All parameters where collected for 313 different products, for which all 19 features where available. The resulting dataset is a mixture of continous and categorical variables. This dataset was used as the master dataset. Later on records from the feedback system where joined with the master dataset.</p>

<figure class="image">
    <p align="center">
        <img src="2021-09-29-16-17-01.png" alt="Data flow framework" width="500" />     
    </p>
    <div style="text-align:center">
        <figcaption class="figure-caption text-center"><i>Data flow framework</i>
        </figcaption>
    </div>
</figure>

<h2 id="exploratory-data-analysis-eda">Exploratory Data Analysis (EDA)</h2>

<p>Data analysis is a crucial step in any Machine Learning problem, is a way to introduce and understand data before moving further to the modeling process. Data analysis provides the analyst with important information about data types, relation between variables in a dataset, their shape, form, distribution as well as descriptive statistics. In data analysis patterns in the data can be discovered by means of numerical tests and visualization techniques. This is in fact one of the most time-consuming steps on any project, since it requires data to be consolidated and prepared for the analysis.</p>

<table>
<thead>
  <tr>
    <th>Correlation Matrix</th>
    <th>Data distribution</th>
  </tr>
</thead>
<tbody>
  <tr>
    <td><img src="2021-09-29-16-33-39.png" alt="matrix" width="400" /></td>
    <td rowspan="3"><img src="raincloud.png" alt="raincloud" width="600" /></td>
  </tr>
  <tr>
    <th>Analysis of residuals</th>
  </tr>
  <tr>
    <td><img src="residuals.png" alt="residuals" width="400" /></td>
  </tr>
</tbody>
</table>

<h2 id="model-evaluation">Model evaluation</h2>
<p>There are numerous regression algorithms already implemented in libraries like <code class="language-plaintext highlighter-rouge">scikit-learn</code> that one can choose from. I chose a simple linear regression models as my baseline model, since it is fast to train due to its vectorized closed form implementation, and easy to interpret. After getting first results and model accurary a set of different models where tested on the same dataset to create a benchmark.</p>

<table>
  <thead>
    <tr>
      <th>Model</th>
      <th>MAE</th>
      <th>MSE</th>
      <th>RMSE</th>
      <th>R2</th>
      <th>RMSLE</th>
      <th>MAPE</th>
      <th>TT (Sec)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>CatBoost Regressor</td>
      <td>4.2354</td>
      <td>50.8014</td>
      <td>6.9793</td>
      <td>0.8645</td>
      <td>0.2672</td>
      <td>0.2042</td>
      <td>1.618</td>
    </tr>
    <tr>
      <td>Extra Trees Regressor</td>
      <td>4.3761</td>
      <td>56.3788</td>
      <td>7.2692</td>
      <td>0.8534</td>
      <td>0.2778</td>
      <td>0.2097</td>
      <td>0.029</td>
    </tr>
    <tr>
      <td>Random Forest Regressor</td>
      <td>4.4652</td>
      <td>58.7568</td>
      <td>7.4328</td>
      <td>0.8481</td>
      <td>0.2661</td>
      <td>0.2050</td>
      <td>0.038</td>
    </tr>
    <tr>
      <td>Gradient Boosting Regressor</td>
      <td>4.5829</td>
      <td>60.1312</td>
      <td>7.5515</td>
      <td>0.8387</td>
      <td>0.2703</td>
      <td>0.2183</td>
      <td>0.014</td>
    </tr>
    <tr>
      <td>Light Gradient Boosting Machine</td>
      <td>4.5967</td>
      <td>66.1371</td>
      <td>7.8272</td>
      <td>0.8363</td>
      <td>0.2817</td>
      <td>0.2099</td>
      <td>0.008</td>
    </tr>
    <tr>
      <td>Extreme Gradient Boosting</td>
      <td>4.5624</td>
      <td>67.4343</td>
      <td>7.9526</td>
      <td>0.8192</td>
      <td>0.2789</td>
      <td>0.2110</td>
      <td>12.492</td>
    </tr>
    <tr>
      <td>AdaBoost Regressor</td>
      <td>6.6773</td>
      <td>81.7521</td>
      <td>8.9387</td>
      <td>0.7735</td>
      <td>0.3999</td>
      <td>0.4283</td>
      <td>0.012</td>
    </tr>
    <tr>
      <td>Decision Tree Regressor</td>
      <td>5.2150</td>
      <td>87.8739</td>
      <td>9.1760</td>
      <td>0.7619</td>
      <td>0.3138</td>
      <td>0.2339</td>
      <td>0.004</td>
    </tr>
    <tr>
      <td>K Neighbors Regressor</td>
      <td>5.3629</td>
      <td>88.5753</td>
      <td>9.2931</td>
      <td>0.7544</td>
      <td>0.3071</td>
      <td>0.2343</td>
      <td>0.005</td>
    </tr>
    <tr>
      <td>Ridge Regression</td>
      <td>6.6509</td>
      <td>95.4289</td>
      <td>9.5335</td>
      <td>0.7442</td>
      <td>0.3869</td>
      <td>0.3239</td>
      <td>0.004</td>
    </tr>
    <tr>
      <td>Least Angle Regression</td>
      <td>6.6597</td>
      <td>95.6338</td>
      <td>9.5440</td>
      <td>0.7433</td>
      <td>0.3877</td>
      <td>0.3246</td>
      <td>0.005</td>
    </tr>
    <tr>
      <td>Linear Regression</td>
      <td>6.6597</td>
      <td>95.6337</td>
      <td>9.5440</td>
      <td>0.7433</td>
      <td>0.3877</td>
      <td>0.3246</td>
      <td>0.004</td>
    </tr>
    <tr>
      <td>Elastic Net</td>
      <td>6.9408</td>
      <td>98.6090</td>
      <td>9.7182</td>
      <td>0.7374</td>
      <td>0.3786</td>
      <td>0.3313</td>
      <td>0.004</td>
    </tr>
    <tr>
      <td>Lasso Regression</td>
      <td>6.9496</td>
      <td>98.9866</td>
      <td>9.7357</td>
      <td>0.7366</td>
      <td>0.3792</td>
      <td>0.3319</td>
      <td>0.004</td>
    </tr>
    <tr>
      <td>Huber Regressor</td>
      <td>6.5085</td>
      <td>100.9395</td>
      <td>9.7533</td>
      <td>0.7321</td>
      <td>0.3367</td>
      <td>0.3023</td>
      <td>0.006</td>
    </tr>
    <tr>
      <td>Bayesian Ridge</td>
      <td>7.0413</td>
      <td>100.8291</td>
      <td>9.8349</td>
      <td>0.7312</td>
      <td>0.3786</td>
      <td>0.3334</td>
      <td>0.004</td>
    </tr>
    <tr>
      <td>Orthogonal Matching Pursuit</td>
      <td>8.0436</td>
      <td>146.5258</td>
      <td>11.7492</td>
      <td>0.6310</td>
      <td>0.3851</td>
      <td>0.3827</td>
      <td>0.004</td>
    </tr>
    <tr>
      <td>Passive Aggressive Regressor</td>
      <td>9.8682</td>
      <td>207.8557</td>
      <td>13.9189</td>
      <td>0.4282</td>
      <td>0.5120</td>
      <td>0.3922</td>
      <td>0.004</td>
    </tr>
    <tr>
      <td>Lasso Least Angle Regression</td>
      <td>15.6397</td>
      <td>406.6446</td>
      <td>19.7724</td>
      <td>-0.0291</td>
      <td>0.7431</td>
      <td>1.0209</td>
      <td>0.004</td>
    </tr>
  </tbody>
</table>

<p>As we can see, benchmarking different models with their standard configurations gives us a pretty good start in terms of accuracy. Accuracy was measured using the <strong>RMSE</strong> (Root Mean Squared Error) metric. These models have still the potential to be optimized. Model optimization goes beyond numerical optimization e.g. <strong>Grid Search</strong> or <strong>Random Search</strong>. Performance of a model can also be improved by carefully performing <strong>feature engineering</strong> and <strong>ensenmbling or stacking</strong> models together. In the table above is possible to see, that relatively simple algorithms like <strong>Random Forest</strong> perform better than more complex and modern approaches like <strong>LGBM (Light Gradient Boosting Machine)</strong>. In fact, the difference in performance between the Random forest Regressor and the CatBoost Regressor is marginal in terms of the RMSE (measured in minutes) one small difference is the training time required for CatBoost is significantly higher, which for the final size of the dataset does not make a lot of difference but if scaled could potentially present an issue.</p>

<h3 id="hyperparameter-optimization">Hyperparameter Optimization</h3>

<p>Since Random Forest is supported natively by <strong>scikit-learn</strong> we can make use of the <strong>GridSearchCV</strong> and <strong>RandomSearhCV</strong> classes to perform Hyperparameter Optimization. The following table show the optimization proocess for the Random Forest Regressor estimator.</p>

<table>
  <thead>
    <tr>
      <th>Hyperparameter</th>
      <th>Value range</th>
      <th>GSearch</th>
      <th>RSearch</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Max. depth</td>
      <td>\(\{None, [2,10] \in \mathbb{Z}\}\)</td>
      <td>8</td>
      <td>None</td>
    </tr>
    <tr>
      <td>Max. features</td>
      <td>\(\{auto, sqrt, log2\}\)</td>
      <td>sqrt</td>
      <td>log2</td>
    </tr>
    <tr>
      <td>Max. leaf nodes</td>
      <td>\(\{None, [2,10] \in \mathbb{Z}\}\)</td>
      <td>None</td>
      <td>None</td>
    </tr>
    <tr>
      <td>Criterion</td>
      <td>\(\{mse, friedeman mse, mae, poisson\}\)</td>
      <td>friedman mse</td>
      <td>mae</td>
    </tr>
    <tr>
      <td>ccp alpha</td>
      <td>\([0, 0.9] \in \mathbb{R}\)</td>
      <td>0.1</td>
      <td>0</td>
    </tr>
  </tbody>
</table>

<p>First a <strong>hyperparameter space</strong> was defined. This set of parameter combinations are used by either constructing the cartesian product of all combinations in case of Grid Search. Which is a Brute force approach to optimization where all possible combinations are evaluated. Or using <strong>Random Search</strong>, which randomly (in this case) samples from the hyperparameter space. The results with a 10-fold cross validation are summarized below.</p>

<table>
  <thead>
    <tr>
      <th><strong>Metric</strong></th>
      <th><strong>Resources</strong></th>
      <th>GSearch</th>
      <th>RSearch</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>\(R^{2}\) with \(CV = 10\)</td>
      <td>Performed on a machine  with 12 CPU cores at 4.3 Ghz and  16 Gb of RAM</td>
      <td>0.8737</td>
      <td>0.8723</td>
    </tr>
    <tr>
      <td>\(RMSE\) with \(CV = 10\)</td>
      <td> </td>
      <td><strong>6.510</strong></td>
      <td><strong>6.541</strong></td>
    </tr>
    <tr>
      <td>Number of fits with \(CV = 10\)</td>
      <td> </td>
      <td>48600</td>
      <td>2500</td>
    </tr>
    <tr>
      <td>Compute time (seconds)</td>
      <td> </td>
      <td>768</td>
      <td>40.3</td>
    </tr>
  </tbody>
</table>

<p>Taking a look at the results after cross validation is possible to see that, the optimization using either Grid Search or Random Search improved the performance of the model by almost over a minute in reduction in the RMSE (See baseline/unoptimized model in table below). Grid Search uses more ressources since it has to fit a larger number of combinations. Random search from the other side requires a fraction of the number of repetitions needed by Grid Search, thus achieving a similar result. At this point there is not much that one could do to the model in terms of hyperparameter optimization. The next step was to see if modifying the feature space could result in an increase of performance.</p>

<table>
  <thead>
    <tr>
      <th><strong>Baseline model</strong></th>
      <th>\(\mathbf{RMSE}\)</th>
      <th>\(\mathbf{R^2}\)</th>
      <th>\(\mathbf{MAE}\)</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>Random Forest regressor</td>
      <td>7.4328</td>
      <td>0.8481</td>
      <td>4.4652</td>
    </tr>
  </tbody>
</table>

<h3 id="dimensionality-reduction">Dimensionality reduction</h3>

<p>For the dataset multiple dimensionality reduction (PCA, LASSO) and feature selection techniques (Model based selection, Recursive feature elimination)</p>

<p>…..</p>

<p>UPLOADING OF THIS CONTENT IS STILL UNDER CONTRUCTION AND WILL BE ONLINE SOON</p>

<p><img src="https://upload.wikimedia.org/wikipedia/commons/thumb/a/a5/Wikidata_logo_under_construction_sign_wide.svg/800px-Wikidata_logo_under_construction_sign_wide.svg.png" alt="under contruction" /></p>




      <footer class="site-footer">
        
          <span class="site-footer-owner"><a href="https://github.com/ealvarezj/DataScience-Portfolio">DataScience-Portfolio</a> is maintained by <a href="https://github.com/ealvarezj">ealvarezj</a>.</span>
        
        <span class="site-footer-credits">This page was generated by <a href="https://pages.github.com">GitHub Pages</a>.</span>
      </footer>
    </main>
  </body>
</html>

